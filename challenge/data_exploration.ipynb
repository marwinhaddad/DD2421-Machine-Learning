{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, RandomizedSearchCV, RepeatedStratifiedKFold, train_test_split\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('TrainOnMe_encoded.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(df: pd.DataFrame, ZSCORE_THRESHOLD: float=3.0):\n",
    "    z_scores = stats.zscore(df.select_dtypes(include=[np.number]))\n",
    "    filtered_entries = (np.abs(z_scores) < ZSCORE_THRESHOLD).all(axis=1)\n",
    "    df = df[filtered_entries]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_y_dict = dict(zip(df['y'].unique(), range(len(df['y'].unique()))))\n",
    "decoder_y_dict = {value: key for key, value in encoder_y_dict.items()}\n",
    "\n",
    "encoder_x7_dict = dict(zip(df['x7'].unique(), range(len(df['x7'].unique()))))\n",
    "decoder_x7_dict = {value: key for key, value in encoder_x7_dict.items()}\n",
    "\n",
    "def encoder(df: pd.DataFrame):\n",
    "    df['y'] = df['y'].map(encoder_y_dict)\n",
    "    df['x7'] = df['x7'].map(encoder_x7_dict)\n",
    "    return df\n",
    "\n",
    "def decoder(df: pd.DataFrame):\n",
    "    df['y'] = df['y'].map(decoder_y_dict)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipe(X: pd.DataFrame, y: pd.DataFrame, pipeline: Pipeline, param_grid: dict, n_splits: int = 10, scoring: str='accuracy', \n",
    "         n_iter: bool = None, shuffle: bool = True, verbose: int = 1, repeated: bool = False, n_repeats: int = 10):\n",
    "\n",
    "    if repeated:\n",
    "        cv = RepeatedStratifiedKFold(n_splits=n_splits, shuffle=shuffle, n_repeats=n_repeats)\n",
    "    else:\n",
    "        cv = StratifiedKFold(n_splits=n_splits, shuffle=shuffle)\n",
    "\n",
    "    if n_iter:\n",
    "        search = RandomizedSearchCV(pipeline, param_grid, n_iter=n_iter, scoring=scoring, cv=cv, verbose=verbose, n_jobs=-1)\n",
    "    else:\n",
    "        search = GridSearchCV(pipeline, param_grid, scoring=scoring, cv=cv, verbose=verbose, n_jobs=-1)\n",
    "\n",
    "    search.fit(X, y)\n",
    "\n",
    "    return search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pre_processing = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('pca', PCA(n_components='mle'))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_processing = Pipeline([\n",
    "    ('scaler', StandardScaler(), df.select_dtypes(include=['float64']).columns)\n",
    "    ('encoder', encoder(df=df)),\n",
    "    ('remove_outliers', remove_outliers(df=df)),\n",
    "    ('pca', PCA(n_components='mle'))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Classifier Pipeline and Hyperparameters\n",
    "lr = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('logistic', LogisticRegression())\n",
    "])\n",
    "\n",
    "lr_param_grid = {\n",
    "    'logistic__penalty': ['l1', 'l2', 'elasticnet', 'none'],\n",
    "    'logistic__C': [0.001, 0.01, 0.1, 1.0, 10.0, 100.0],\n",
    "    'logistic__solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n",
    "}\n",
    "\n",
    "lr_model = {'model': lr, 'param_grid': lr_param_grid}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Classifier Pipeline and Parameters\n",
    "rf_pipeline = Pipeline([\n",
    "    ('pre_processing', pre_processing),\n",
    "    ('rf', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "rf_param_grid = {\n",
    "    'rf__n_estimators': [100, 200, 300, 400, 500],\n",
    "    'rf__max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None],\n",
    "    'rf__min_samples_split': [2, 5, 10],\n",
    "    'rf__min_samples_leaf': [1, 2, 4],\n",
    "    'rf__bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "rf_model = {'model': rf_pipeline, 'param_grid': rf_param_grid}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost Classifier Pipeline and Parameters\n",
    "xgb_pipeline = Pipeline([\n",
    "    ('pre_processing', pre_processing),\n",
    "    ('xgb', XGBClassifier())\n",
    "])\n",
    "\n",
    "xgb_param_grid = {\n",
    "    'xgb__n_estimators': [100, 200, 300, 400, 500],\n",
    "    'xgb__max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None],\n",
    "    'xgb__learning_rate': [0.1, 0.01, 0.001],\n",
    "    'xgb__subsample': [0.5, 0.7, 1.0],\n",
    "    'xgb__colsample_bytree': [0.5, 0.7, 1.0],\n",
    "    'xgb__gamma': [0, 1, 5]\n",
    "}\n",
    "\n",
    "xgb_model = {'model': xgb_pipeline, 'param_grid': xgb_param_grid}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_pipeline = Pipeline(steps=[\n",
    "    ('pre_processing', pre_processing),\n",
    "    ('ensemble',    VotingClassifier(estimators=[\n",
    "                        ('lr', LogisticRegression()),\n",
    "                        ('rf', RandomForestClassifier()),\n",
    "                        ('xgb', XGBClassifier())\n",
    "                    ]))\n",
    "])\n",
    "\n",
    "ensemble_param_grid = {\n",
    "    'ensemble__voting': ['hard', 'soft'],\n",
    "    'ensemble__lr__C': [0.1, 1.0, 10.0],\n",
    "    'ensemble__lr__solver': ['liblinear', 'lbfgs'],\n",
    "    'ensemble__rf__n_estimators': [10, 50, 100],\n",
    "    'ensemble__rf__max_depth': [None, 10, 20],\n",
    "    'ensemble__xgb__learning_rate': [0.01, 0.1, 0.3],\n",
    "    'ensemble__xgb__max_depth': [3, 6, 9]\n",
    "}\n",
    "\n",
    "ensamble_model = {'model': main_pipeline, 'param_grid': ensemble_param_grid}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models = {  'lr': lr_model, \n",
    "                'rf': rf_model, \n",
    "                'xgb': xgb_model, \n",
    "                'ensemble': ensamble_model\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training lr model...\n",
      "Fitting 10 folds for each of 10 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\marwi\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:425: FitFailedWarning: \n",
      "30 fits failed out of a total of 100.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "10 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\marwi\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\marwi\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\marwi\\anaconda3\\Lib\\site-packages\\sklearn\\pipeline.py\", line 420, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"c:\\Users\\marwi\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\marwi\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1168, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\marwi\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 56, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "10 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\marwi\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\marwi\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\marwi\\anaconda3\\Lib\\site-packages\\sklearn\\pipeline.py\", line 420, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"c:\\Users\\marwi\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\marwi\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1168, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\marwi\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 66, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "10 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\marwi\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\marwi\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\marwi\\anaconda3\\Lib\\site-packages\\sklearn\\pipeline.py\", line 420, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"c:\\Users\\marwi\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\marwi\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1168, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\marwi\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 56, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\marwi\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:976: UserWarning: One or more of the test scores are non-finite: [0.6025  0.6575      nan     nan 0.77    0.66375 0.77    0.76625 0.68375\n",
      "     nan]\n",
      "  warnings.warn(\n",
      "c:\\Users\\marwi\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\marwi\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1192: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best lr model: {'logistic__solver': 'newton-cg', 'logistic__penalty': 'none', 'logistic__C': 0.01}\n",
      "Best lr model score: 0.7699999999999999\n",
      "Training rf model...\n",
      "Fitting 10 folds for each of 10 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\marwi\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\marwi\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1192: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best rf model: {'rf__n_estimators': 400, 'rf__min_samples_split': 2, 'rf__min_samples_leaf': 2, 'rf__max_depth': 10, 'rf__bootstrap': False}\n",
      "Best rf model score: 0.8387499999999999\n",
      "Training xgb model...\n",
      "Fitting 10 folds for each of 10 candidates, totalling 100 fits\n",
      "Best xgb model: {'xgb__subsample': 0.7, 'xgb__n_estimators': 500, 'xgb__max_depth': 20, 'xgb__learning_rate': 0.01, 'xgb__gamma': 1, 'xgb__colsample_bytree': 1.0}\n",
      "Best xgb model score: 0.8574999999999999\n",
      "Training ensemble model...\n",
      "Fitting 10 folds for each of 10 candidates, totalling 100 fits\n",
      "Best ensemble model: {'ensemble__xgb__max_depth': 6, 'ensemble__xgb__learning_rate': 0.1, 'ensemble__voting': 'soft', 'ensemble__rf__n_estimators': 100, 'ensemble__rf__max_depth': 10, 'ensemble__lr__solver': 'liblinear', 'ensemble__lr__C': 1.0}\n",
      "Best ensemble model score: 0.8637499999999999\n"
     ]
    }
   ],
   "source": [
    "X, y = df.drop('y', axis=1), df['y']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "for model_name, model in all_models.items():\n",
    "    print(f\"Training {model_name} model...\")\n",
    "    \n",
    "    search = pipe(X=X_train, y=y_train, pipeline=model['model'], param_grid=model['param_grid'], n_iter=10)\n",
    "\n",
    "    print(f\"Best {model_name} model: {search.best_params_}\")\n",
    "    print(f\"Best {model_name} model score: {search.best_score_}\")\n",
    "\n",
    "    model['model'].set_params(**search.best_params_)\n",
    "    model['model'].fit(X_train, y_train)\n",
    "    y_pred = model['model'].predict(X_test)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      1.00      0.92        55\n",
      "           1       0.90      0.93      0.91        80\n",
      "           2       0.91      0.75      0.82        65\n",
      "\n",
      "    accuracy                           0.89       200\n",
      "   macro avg       0.89      0.89      0.89       200\n",
      "weighted avg       0.89      0.89      0.89       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
